# Next Word Prediction: Sherlock Holmes (Bi-LSTM Model)

This project is a deep learning model for Natural Language Processing (NLP) that predicts the next word in a sequence. It is built with Keras (TensorFlow) and trained on the "Adventures of Sherlock Holmes" corpus.

This model uses a **Bidirectional LSTM (Bi-LSTM)** architecture, which allows it to learn the context from both forward and backward directions, leading to more accurate and contextually aware text generation.

**Final Training Accuracy: 82.56%**

---

## üß† Model Architecture

The model was built using the Keras `Sequential` API. The architecture was designed to be memory-efficient for training on standard environments like Google Colab.

| Layer (type) | Output Shape | Param # | Purpose |
| :--- | :--- | :--- | :--- |
| `Embedding` | (None, 50, 100) | 822,500 | Converts 8,225 unique words into 100-dimension vectors. |
| `Bidirectional(LSTM)` | (None, 300) | 301,200 | (150 units x 2 directions) Reads the 50-word sequence forward and backward to understand context. |
| `Dropout` | (None, 300) | 0 | Prevents overfitting by randomly dropping 20% of connections. |
| `Dense (Output)` | (None, 8225) | 2,475,725 | `softmax` layer that outputs a probability for each of the 8,225 words. |
| **Total** | | **3,599,425** | **Total Trainable Parameters** |

---

## üìö Dataset

The model was trained on the complete text of **"The Adventures of Sherlock Holmes"** by Arthur Conan Doyle (Project Gutenberg).

* **Vocabulary Size:** 8,225 unique words
* **Input Sequence Length:** 50 words
* **Total Training Sequences:** 101,138

---

## üöÄ How to Use (Prediction)

You don't need to retrain the model. You can use the pre-trained files in this repository (`.h5` and `.pkl`) to generate text immediately.

1.  **Clone or Download:** Get the `next_word_model_BILSTM.h5`, `tokenizer.pkl`, and the `.ipynb` notebook.
2.  **Upload to Colab:** Open the notebook in Google Colab and upload both `.h5` and `.pkl` files to the session storage üìÅ.
3.  **Run Prediction:** Run the cells in the notebook required to load the model and tokenizer, then use the prediction function.

### Sample Prediction:

Here is an actual sample generated by the model:

**Seed Text:** `sherlock holmes looked at`
**Model Prediction:** `sherlock holmes looked at him in his chair and the man who had been`

## üöÄ Download the Trained Model

The trained model file (`.h5`) and the tokenizer (`.pkl`) are too large to be hosted directly on GitHub (over 100MB).

You can download the pre-trained assets from Google Drive:

**[‚û°Ô∏è Download Model (.h5) and Tokenizer (.pkl) from Google Drive]([YOUR_GOOGLE_DRIVE_SHARE_LINK_HERE](https://drive.google.com/file/d/1yNK309DDDR_2_uh5-0g-ReYxMRnuwjSM/view?usp=drive_link))**

*(**Note:** You must get the shareable link from your Google Drive and replace "YOUR_GOOGLE_DRIVE_SHARE_LINK_HERE" with your actual link. Make sure the link is set to "Anyone with the link can view".)*
