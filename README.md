# Next Word Prediction: Sherlock Holmes (Bi-LSTM Model)

This project is a deep learning model for Natural Language Processing (NLP) that predicts the next word in a sequence. It is built with Keras (TensorFlow) and trained on the "Adventures of Sherlock Holmes" corpus.

This model uses a **Bidirectional LSTM (Bi-LSTM)** architecture, which allows it to learn the context from both forward and backward directions, leading to more accurate and contextually aware text generation.

**Final Training Accuracy: 82.56%**

---

## üß† Model Architecture

The model was built using the Keras `Sequential` API. The architecture was designed to be memory-efficient for training on standard environments like Google Colab.

| Layer (type) | Output Shape | Param # | Purpose |
| :--- | :--- | :--- | :--- |
| `Embedding` | (None, 50, 100) | 822,500 | Converts 8,225 unique words into 100-dimension vectors. |
| `Bidirectional(LSTM)` | (None, 300) | 301,200 | (150 units x 2 directions) Reads the 50-word sequence forward and backward to understand context. |
| `Dropout` | (None, 300) | 0 | Prevents overfitting by randomly dropping 20% of connections. |
| `Dense (Output)` | (None, 8225) | 2,475,725 | `softmax` layer that outputs a probability for each of the 8,225 words. |
| **Total** | | **3,599,425** | **Total Trainable Parameters** |

---

## üìö Dataset

The model was trained on the complete text of **"The Adventures of Sherlock Holmes"** by Arthur Conan Doyle (Project Gutenberg).

* **Vocabulary Size:** 8,225 unique words
* **Input Sequence Length:** 50 words
* **Total Training Sequences:** 101,138

---

## üöÄ How to Use (Prediction)

You don't need to retrain the model. You can use the pre-trained files in this repository (`.h5` and `.pkl`) to generate text immediately.

1.  **Clone or Download:** Get the `next_word_model_BILSTM.h5`, `tokenizer.pkl`, and the `.ipynb` notebook.
2.  **Upload to Colab:** Open the notebook in Google Colab and upload both `.h5` and `.pkl` files to the session storage üìÅ.
3.  **Run Prediction:** Run the cells in the notebook required to load the model and tokenizer, then use the prediction function.

### Sample Prediction:

Here is an actual sample generated by the model:

**Seed Text:** `sherlock holmes looked at`
**Model Prediction:** `sherlock holmes looked at him in his chair and the man who had been`
